{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Main Sections:\n",
    "\n",
    "1. TensorFlow Basics:\n",
    "    * Introduction to TensorFlow\n",
    "    * Installing and Importing TensorFlow\n",
    "    * Constants and Variables\n",
    "    * Simple Operations and Tensors\n",
    "    * Placeholders and Data Feeding\n",
    "2. Neural Network Fundamentals:\n",
    "    * Building a Linear Regression Model\n",
    "    * Loss Functions and Optimization\n",
    "    * Activation Functions\n",
    "    * Training Loop and Metrics\n",
    "3. Hyperparameter Tuning:\n",
    "    * Importance of Hyperparameters\n",
    "    * Common Hyperparameters (batch size, epoch, learning rate)\n",
    "    * GridSearchCV and RandomSearch for Exploration\n",
    "    * Visualizing Tuning Results\n",
    "4. Deep Learning Layers:\n",
    "    * Dense Layers: Parameters and Activation Functions\n",
    "    * Dropout Layers for Regularization\n",
    "    * Additional Layer Types (convolutional, recurrent, pooling)\n",
    "    * Compile Parameters and Training Configuration\n",
    "5. Additional Notes:\n",
    "    * Code snippets and resources for further learning\n",
    "    * Specific questions or challenges you encountered\n",
    "\n",
    "### Sub-Sections (optional):\n",
    "    * Within each main section, you can create sub-sections for specific topics or examples.\n",
    "    * Consider adding headers, bullet points, and diagrams for better organization and clarity.\n",
    "    * Use different colors or fonts to highlight important information.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# special imports for CV and NLP\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "print(tf.__version__)\n",
    "# 2.7.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helping Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 2\n",
    "BATCH_SIZE = 32\n",
    "SEED = 1\n",
    "\n",
    "IMG_HEIGHT = 300\n",
    "IMG_WIDTH = 300\n",
    "CHANNELS = 3\n",
    "\n",
    "num_inputs = 1\n",
    "num_classes = 3\n",
    "\n",
    "X_train = np.array([1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 8.0, 9.0, 10.0], dtype=float)\n",
    "y_train = np.array([1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.5, 5.0, 5.5], dtype=float)\n",
    "\n",
    "X_test = np.array([7.0, 11.0], dtype=float)\n",
    "y_test = np.array([4.0, 6.0], dtype=float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get time series data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get some time series data\n",
    "# the original code is taken from:\n",
    "# https://github.com/https-deeplearning-ai/tensorflow-1-public\n",
    "\n",
    "def plot_series(time, series, format=\"-\", start=0, end=None):\n",
    "    plt.plot(time[start:end], series[start:end], format)\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.grid(True)\n",
    "\n",
    "\n",
    "def trend(time, slope=0):\n",
    "    return slope * time\n",
    "\n",
    "\n",
    "def seasonal_pattern(season_time):\n",
    "    \"\"\" Just an arbitrary pattern \"\"\"\n",
    "    return np.where(season_time < 0.3,\n",
    "                    np.sin(season_time * 4 * np.pi),\n",
    "                    1 / np.exp(7 * season_time))\n",
    "\n",
    "\n",
    "def seasonality(time, period, amplitude=1, phase=0):\n",
    "    \"\"\" Repeats the same pattern at each period \"\"\"\n",
    "    season_time = ((time + phase) % period) / period\n",
    "    return amplitude * seasonal_pattern(season_time)\n",
    "\n",
    "\n",
    "def noise(time, noise_level=1, seed=None):\n",
    "    rnd = np.random.RandomState(seed)\n",
    "    return rnd.randn(len(time)) * noise_level\n",
    "\n",
    "\n",
    "time = np.arange(4 * 365 + 1, dtype=\"float32\")\n",
    "baseline = 10\n",
    "series = trend(time, 0.1)\n",
    "baseline = 10\n",
    "amplitude = 40\n",
    "slope = 0.01\n",
    "noise_level = 2\n",
    "\n",
    "# Create the series\n",
    "series = baseline + trend(time, slope) + \\\n",
    "    seasonality(time, period=365, amplitude=amplitude)\n",
    "# Update with noise\n",
    "series += noise(time, noise_level, seed=42)\n",
    "\n",
    "split_time = 3000\n",
    "time_train = time[:split_time]\n",
    "series_train = series[:split_time]\n",
    "\n",
    "WINDOW_SIZE = 20\n",
    "SHUFFLE_BUFFER_SIZE = 1000\n",
    "\n",
    "plot_series(time, series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(series)\n",
    "    dataset = dataset.window(window_size + 1, shift=1, drop_remainder=True)\n",
    "    dataset = dataset.flat_map(lambda window: window.batch(window_size + 1))\n",
    "    dataset = dataset.shuffle(shuffle_buffer).map(\n",
    "        lambda window: (window[:-1], window[-1]))\n",
    "    dataset = dataset.batch(batch_size).prefetch(1)\n",
    "    return dataset\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(51)\n",
    "np.random.seed(51)\n",
    "\n",
    "dataset = windowed_dataset(series_train,\n",
    "                           WINDOW_SIZE,\n",
    "                           BATCH_SIZE,\n",
    "                           SHUFFLE_BUFFER_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ImageDataGenerator\n",
    "\n",
    "Official documentation - https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract Horse or Human datasets for computer vision (ImageDataGenerator)\n",
    "# downloaded from: https://laurencemoroney.com/datasets.html\n",
    "\n",
    "import os\n",
    "import zipfile\n",
    "import shutil\n",
    "\n",
    "# Get the Horse or Human dataset\n",
    "path_horse_or_human = \"data/horse-or-human.zip\"\n",
    "# Get the Horse or Human Validation dataset\n",
    "path_validation_horse_or_human = \"data/validation-horse-or-human.zip\"\n",
    "\n",
    "local_zip = path_horse_or_human\n",
    "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
    "zip_ref.extractall('data/horse-or-human')\n",
    "zip_ref.close()\n",
    "\n",
    "local_zip = path_validation_horse_or_human\n",
    "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
    "zip_ref.extractall('data/validation-horse-or-human')\n",
    "zip_ref.close()\n",
    "\n",
    "train_dir = 'data/horse-or-human'\n",
    "validation_dir = 'data/validation-horse-or-human'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# augmented ImageDataGenerator\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest')\n",
    "\n",
    "# default ImageDataGenerator without augmentation\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary'  # 'categorical'\n",
    ")\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    validation_dir,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary'  # 'categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Found 1027 images belonging to 2 classes.\n",
    "\n",
    "Found 256 images belonging to 2 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Another option is not to specify the validation set explicitly,\n",
    "# but to entrust the split to ImageDataGenerator\n",
    "# In this case, be sure that the seed parameter is the same\n",
    "# for both sets, otherwise the sets may overlap\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                   # set validation split ratio\n",
    "                                   validation_split=0.2)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary',  # 'categorical'\n",
    "    subset='training',  # set as training data\n",
    "    seed=SEED)\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,  # the same directory\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary',  # 'categorical'\n",
    "    subset='validation',  # set as validation data\n",
    "    seed=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Found 822 images belonging to 2 classes.\n",
    "\n",
    "Found 205 images belonging to 2 classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize and pad sentences\n",
    "\n",
    "\n",
    "Official documentation for Tokenizer - https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/TokenizerOfficial documentation for pad_sequences method - https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nerual Netowrk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A neural network model is a collection of layers.\n",
    "\n",
    "A layer is a defined set of computations which takes a given tensor as input and produces another tensor as output.\n",
    "\n",
    "For example, a simple layer could just add 1 to all the elements of an input tensor.\n",
    "\n",
    "The important point is that a layer manipulates (performs a mathematical operation on) an input tensor in some way to produce an output tensor.\n",
    "\n",
    "Combine a series of layers together and you have a model.\n",
    "\n",
    "The term “deep learning” comes from the stacking of large numbers of layers on top of eachother (deep in this sense is a synonym for large).\n",
    "\n",
    "The best way to stack layers together to find patterns in data is constantly changing.\n",
    "\n",
    "This is why techniques such as *transfer learning* are helpful because they allow you to leverage what has worked for someone else’s similar problem and tailor it to your own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layers\n",
    "create layers using the `tf.keras.layers` module.\n",
    "\n",
    "Example include [Dense(fully connected)] and [Conv2D(convolutional)] layes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# Create a dense (or fully connected) layer with TensorFlow\n",
    "dense_layer = tf.keras.layers.Dense(units,  activation=tf.keras.activations.relu)\n",
    "\n",
    "# Create a 2D convolutional layer with TensorFlow\n",
    "conv2d_layer = tf.keras.layers.Conv2D(filters, kernel_size, activation=tf.keras.activations.relu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there are many more pre-built layer types available in the [TensorFlow documentation](https://www.tensorflow.org/api_docs/python/tf/keras/layers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Models in TensorFlow\n",
    "The most straightforward way in TensorFlow to a neural network model us using the `tf.keras.Sequential` API, which allows you to stack layers in a way that the computation will be performed sequentially (one layer after the other):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a basic neural network model with TensorFlow\n",
    "model = models.Sequential()\n",
    "\n",
    "#\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape),\n",
    "    # Add layers here\n",
    "\t\t# Computation will happen layer by layer sequentially\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import datasets, layers, models\n",
    "# Creating the Layers\n",
    "\n",
    "\n",
    "# for Neural Network Model\n",
    "model.add(layers.Flatten(input_shape=(28, 28)))\n",
    "model.add(layers.Dense(512, activation=tf.nn.relu))\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.Dense(10, activation=tf.nn.softmax))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 1. Typical Neural Network Architectures using Sequential API\n",
    "\n",
    "Official documentation - https://www.tensorflow.org/guide/keras/sequential_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # DNN\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(units=64, activation='relu',\n",
    "                          input_shape=(num_inputs, )),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(units=32, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(units=1, activation='relu'),  # regression\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # CNN\n",
    "model = tf.keras.models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu',\n",
    "                  input_shape=(IMG_HEIGHT, IMG_WIDTH, CHANNELS)),\n",
    "    layers.MaxPooling2D(2, 2),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D(2, 2),\n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D(2, 2),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(1, activation='sigmoid')  # binary classification\n",
    "    # multi-class classification\n",
    "    # layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # RNN for NLP\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Embedding(vocab_size,\n",
    "                     embedding_dim,\n",
    "                     input_length=max_length),\n",
    "    layers.Bidirectional(layers.LSTM(32, return_sequences=True)),\n",
    "    layers.Bidirectional(layers.LSTM(16)),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(16, activation='relu'),\n",
    "    layers.Dense(6, activation='softmax')  # multiclass classification\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN for time series\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Conv1D(filters=128, kernel_size=7,\n",
    "                  strides=1, padding=\"causal\",\n",
    "                  activation=\"relu\",\n",
    "                  input_shape=(WINDOW_SIZE, 1)),\n",
    "    # univariate time series - predict a value based on\n",
    "    # 'WINDOW_SIZE' previous steps for 1 feature\n",
    "\n",
    "    layers.LSTM(32, return_sequences=True),\n",
    "    layers.LSTM(16, return_sequences=True),\n",
    "    layers.Dense(16, activation=\"relu\"),\n",
    "    layers.Dense(1)  # predict one value\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile Model\n",
    "Before we start training the model on data we’ve got to compile it model with an optimizer, loss function, and metric.\n",
    "\n",
    "All three of these are customizable depending on the project you’re working on.\n",
    "\n",
    "- **Loss function** = measures how wrong the model is (the higher the loss, the more wrong the model, so lower is better). Common loss values include `tf.keras.losses.CategoricalCrossentropy` for multi-class classification problems and `tf.keras.losses.BinaryCrossentropy` for binary classification problems.\n",
    "\n",
    "- **Optimizer** = tries to adjust the models parameters to lower the loss value. Common optimizers include `tf.keras.optimizers.SGD` (Stochastic Gradient Descent) and `tf.keras.optimizers.Adam`. Each optimizer comes with generally good default settings, however, of the most important values to set is the learning_rate hyperparameter.\n",
    "\n",
    "- **Metric** = human readable value to interpret how the model is going. Metrics can be defined via `tf.keras.metrics` such as `tf.keras.metrics.Accuracy` or via a list of strings such as `['accuracy']`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model before training\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), \n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model\n",
    "You can train your TensorFlow models using the `tf.keras.Model.fit` method and passing it the appropriate hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model to a set of data\n",
    "model.fit(x=X_train, # data to find patterns in \n",
    "          y=y_train, # labels for x\n",
    "          epochs=1, # how many times to go over the whole dataset\n",
    "          batch_size=32, # number of samples to look at each time\n",
    "          validation_data=(X_val, # data to validate the learned patterns\n",
    "                           y_val)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model\n",
    "Once you’ve trained a model, you can evaluate its performance on unseen data with `tf.keras.Model.evaluate`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate a model on unseen data\n",
    "loss, accuracy = model.evaluate(x=X_test, \n",
    "                                y=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save and Load a Whole TensorFlow Model\n",
    "There are two main ways to save a whole TensorFlow model:\n",
    "\n",
    "- **`SavedModel` format** — default format for saving a model in TensorFlow, saves a complete TensorFlow program including trained parameters and does not require the original model building code to run. Useful for sharing models across TensorFlow.js, TFLite, TensorFlow Serving and more.\n",
    "\n",
    "- **`HDF5` format** — a more widely used data standard, however, does not contain as much information as the `SavedModel` format.\n",
    "\n",
    "\n",
    "You can read the pros and cons of each of these in the TensorFlow documentation on Save and load Keras models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model to SavedModel format (default)\n",
    "model.save('model_save_path')\n",
    "\n",
    "# Load model back in from SavedModel format\n",
    "loaded_model = tf.keras.models.load_model('model_save_path')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
